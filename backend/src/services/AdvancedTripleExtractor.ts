import * as natural from 'natural';
const compromise = require('compromise');
import { NewKnowledgeTriple } from './ResponseHandler';
import { ContextBundle } from './ContextOrchestrator';

// NLP ë¶„ì„ ê²°ê³¼ë¥¼ ë‹´ëŠ” ì¸í„°í˜ì´ìŠ¤
export interface NLPAnalysis {
  entities: Entity[];
  relationships: Relationship[];
  dependencies: Dependency[];
  semanticRoles: SemanticRole[];
  confidence: number;
}

export interface Entity {
  text: string;
  label: string; // PERSON, ORG, CONCEPT, etc.
  startIndex: number;
  endIndex: number;
  confidence: number;
  uri?: string; // ì˜¨í†¨ë¡œì§€ URI
}

export interface Relationship {
  subject: Entity;
  predicate: string;
  object: Entity;
  confidence: number;
  context?: string;
}

export interface Dependency {
  token: string;
  head: string;
  relation: string; // nsubj, dobj, prep, etc.
  position: number;
}

export interface SemanticRole {
  predicate: string;
  agent?: Entity;    // ARG0 - í–‰ìœ„ì
  patient?: Entity;  // ARG1 - ëŒ€ìƒ
  instrument?: Entity; // ARG2 - ë„êµ¬
  location?: Entity;   // ARGM-LOC - ì¥ì†Œ
  time?: Entity;       // ARGM-TMP - ì‹œê°„
  manner?: Entity;     // ARGM-MNR - ë°©ì‹
}

/**
 * ê³ ê¸‰ NLP ê¸°ë²•ì„ ì‚¬ìš©í•œ RDF íŠ¸ë¦¬í”Œ ì¶”ì¶œ ì„œë¹„ìŠ¤
 * 
 * ê¸°ëŠ¥:
 * 1. Named Entity Recognition (NER)
 * 2. Dependency Parsing
 * 3. Semantic Role Labeling (SRL)
 * 4. Knowledge Graph Alignment
 */
export class AdvancedTripleExtractor {
  private tokenizer: any;
  private stemmer: any;
  private posClassifier: any;

  constructor() {
    this.tokenizer = new natural.WordTokenizer();
    this.stemmer = natural.PorterStemmer;
    this.posClassifier = new natural.BayesClassifier();
    this.initializeNLPModels();
  }

  /**
   * NLP ëª¨ë¸ë“¤ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
   */
  private initializeNLPModels(): void {
    // POS íƒœê±° ì´ˆê¸°í™” (ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜)
    // ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë” ì •êµí•œ ëª¨ë¸ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
    console.log('AdvancedTripleExtractor initialized with NLP models');
  }

  /**
   * í…ìŠ¤íŠ¸ì—ì„œ ê³ ê¸‰ NLP ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
   */
  public async analyzeText(text: string): Promise<NLPAnalysis> {
    try {
      // 1. í† í°í™” ë° ì „ì²˜ë¦¬
      const tokens = this.tokenizer.tokenize(text);
      const cleanText = this.preprocessText(text);

      // 2. Named Entity Recognition
      const entities = await this.extractEntities(cleanText);

      // 3. Dependency Parsing
      const dependencies = await this.parseDependencies(cleanText, tokens);

      // 4. Semantic Role Labeling
      const semanticRoles = await this.labelSemanticRoles(cleanText, entities);

      // 5. Relationship Extraction
      const relationships = await this.extractRelationships(entities, dependencies, semanticRoles);

      // 6. ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°
      const confidence = this.calculateOverallConfidence(entities, relationships, dependencies);

      return {
        entities,
        relationships,
        dependencies,
        semanticRoles,
        confidence
      };
    } catch (error) {
      console.error('NLP ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:', error);
      throw new Error(`NLP analysis failed: ${error}`);
    }
  }

  /**
   * í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
   */
  private preprocessText(text: string): string {
    return text
      .trim()
      .replace(/\s+/g, ' ')                    // ì¤‘ë³µ ê³µë°± ì œê±°
      .replace(/[""'']/g, '"')                 // ì¸ìš©ë¶€í˜¸ ì •ê·œí™”
      .replace(/[â€¦]/g, '...')                  // ë§ì¤„ì„í‘œ ì •ê·œí™”
      .replace(/[â€“â€”]/g, '-');                  // ëŒ€ì‹œ ì •ê·œí™”
  }

  /**
   * Named Entity Recognitionì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
   */
  private async extractEntities(text: string): Promise<Entity[]> {
    const entities: Entity[] = [];

    try {
      // Compromise.jsë¥¼ ì‚¬ìš©í•œ ê¸°ë³¸ NER
      const doc = compromise(text);
      
      // ì‚¬ëŒ ì´ë¦„ ì¶”ì¶œ
      const people = doc.people().out('array');
      people.forEach((person: string, index: number) => {
        const startIndex = text.indexOf(person);
        if (startIndex !== -1) {
          entities.push({
            text: person,
            label: 'PERSON',
            startIndex,
            endIndex: startIndex + person.length,
            confidence: 0.85,
            uri: `habitus33:Person_${this.generateEntityId(person)}`
          });
        }
      });

      // ì¥ì†Œ ì¶”ì¶œ
      const places = doc.places().out('array');
      places.forEach((place: string) => {
        const startIndex = text.indexOf(place);
        if (startIndex !== -1) {
          entities.push({
            text: place,
            label: 'PLACE',
            startIndex,
            endIndex: startIndex + place.length,
            confidence: 0.80,
            uri: `habitus33:Place_${this.generateEntityId(place)}`
          });
        }
      });

      // ì¡°ì§ ì¶”ì¶œ
      const organizations = doc.organizations().out('array');
      organizations.forEach((org: string) => {
        const startIndex = text.indexOf(org);
        if (startIndex !== -1) {
          entities.push({
            text: org,
            label: 'ORGANIZATION',
            startIndex,
            endIndex: startIndex + org.length,
            confidence: 0.75,
            uri: `habitus33:Organization_${this.generateEntityId(org)}`
          });
        }
      });

      // í•œêµ­ì–´ íŠ¹í™” ê°œë… ì¶”ì¶œ (ê·œì¹™ ê¸°ë°˜)
      entities.push(...this.extractKoreanConcepts(text));

      // ì¤‘ë³µ ì œê±° ë° ì •ë ¬
      return this.deduplicateEntities(entities);

    } catch (error) {
      console.error('Entity extraction ì˜¤ë¥˜:', error);
      return [];
    }
  }

  /**
   * í•œêµ­ì–´ íŠ¹í™” ê°œë…ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
   */
  private extractKoreanConcepts(text: string): Entity[] {
    const concepts: Entity[] = [];
    
    // í•™ìˆ  ê°œë… íŒ¨í„´
    const conceptPatterns = [
      /([ê°€-í£]+(?:í•™|ë¡ |ë²•|ìˆ |ê¸°ë²•|ë°©ë²•|ì´ë¡ |ì›ë¦¬|ê°œë…))/g,
      /([ê°€-í£]+(?:ì‹œìŠ¤í…œ|ëª¨ë¸|í”„ë ˆì„ì›Œí¬|ì•„í‚¤í…ì²˜))/g,
      /([ê°€-í£]+(?:ë¶„ì„|í‰ê°€|ì¸¡ì •|ê²€ì¦|í…ŒìŠ¤íŠ¸))/g,
      /(ì¸ê³µì§€ëŠ¥|ë¨¸ì‹ ëŸ¬ë‹|ë”¥ëŸ¬ë‹|ë°ì´í„°|ì•Œê³ ë¦¬ì¦˜)/g,
      /([ê°€-í£]+(?:ì—°êµ¬|ê°œë°œ|êµ¬í˜„|ì„¤ê³„|ê³„íš))/g
    ];

    conceptPatterns.forEach(pattern => {
      let match;
      while ((match = pattern.exec(text)) !== null) {
        const concept = match[1] || match[0];
        concepts.push({
          text: concept,
          label: 'CONCEPT',
          startIndex: match.index,
          endIndex: match.index + concept.length,
          confidence: 0.70,
          uri: `habitus33:Concept_${this.generateEntityId(concept)}`
        });
      }
    });

    return concepts;
  }

  /**
   * Dependency Parsingì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
   */
  private async parseDependencies(text: string, tokens: string[]): Promise<Dependency[]> {
    const dependencies: Dependency[] = [];

    try {
      // Compromise.jsë¥¼ ì‚¬ìš©í•œ ê¸°ë³¸ êµ¬ë¬¸ ë¶„ì„
      const doc = compromise(text);
      const sentences = doc.sentences().out('array');

      sentences.forEach((sentence: string) => {
        const sentenceDoc = compromise(sentence);
        
        // ë™ì‚¬ ì¶”ì¶œ
        const verbs = sentenceDoc.verbs().out('array');
        
        // ëª…ì‚¬ ì¶”ì¶œ (ì£¼ì–´, ëª©ì ì–´ ì—­í• )
        const nouns = sentenceDoc.nouns().out('array');

        // ê°„ë‹¨í•œ ì£¼ì–´-ë™ì‚¬ ê´€ê³„ (ì²« ë²ˆì§¸ ëª…ì‚¬ê°€ ì£¼ì–´)
        if (nouns.length > 0 && verbs.length > 0) {
          dependencies.push({
            token: nouns[0],
            head: verbs[0],
            relation: 'nsubj',
            position: sentence.indexOf(nouns[0])
          });
        }

        // ë™ì‚¬-ëª©ì ì–´ ê´€ê³„ (ë‘ ë²ˆì§¸ ëª…ì‚¬ê°€ ëª©ì ì–´)
        if (nouns.length > 1 && verbs.length > 0) {
          dependencies.push({
            token: nouns[1],
            head: verbs[0],
            relation: 'dobj',
            position: sentence.indexOf(nouns[1])
          });
        }

        // í•œêµ­ì–´ íŠ¹í™” ì˜ì¡´ì„± íŒ¨í„´
        dependencies.push(...this.extractKoreanDependencies(sentence));
      });

      return dependencies;

    } catch (error) {
      console.error('Dependency parsing ì˜¤ë¥˜:', error);
      return [];
    }
  }

  /**
   * í•œêµ­ì–´ íŠ¹í™” ì˜ì¡´ì„±ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
   */
  private extractKoreanDependencies(sentence: string): Dependency[] {
    const dependencies: Dependency[] = [];

    // í•œêµ­ì–´ ì¡°ì‚¬ íŒ¨í„´ ë¶„ì„
    const particlePatterns = [
      { pattern: /([ê°€-í£]+)ì€\/ëŠ”\s+([ê°€-í£]+)/g, relation: 'topic' },
      { pattern: /([ê°€-í£]+)ì´\/ê°€\s+([ê°€-í£]+)/g, relation: 'subject' },
      { pattern: /([ê°€-í£]+)ì„\/ë¥¼\s+([ê°€-í£]+)/g, relation: 'object' },
      { pattern: /([ê°€-í£]+)ì—ì„œ\s+([ê°€-í£]+)/g, relation: 'location' },
      { pattern: /([ê°€-í£]+)ë¡œ\s+([ê°€-í£]+)/g, relation: 'manner' }
    ];

    particlePatterns.forEach(({ pattern, relation }) => {
      let match;
      while ((match = pattern.exec(sentence)) !== null) {
        dependencies.push({
          token: match[1],
          head: match[2],
          relation,
          position: match.index
        });
      }
    });

    return dependencies;
  }

  /**
   * Semantic Role Labelingì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
   */
  private async labelSemanticRoles(text: string, entities: Entity[]): Promise<SemanticRole[]> {
    const semanticRoles: SemanticRole[] = [];

    try {
      const doc = compromise(text);
      const sentences = doc.sentences().out('array');

      sentences.forEach((sentence: string) => {
        const sentenceDoc = compromise(sentence);
        const verbs = sentenceDoc.verbs().out('array');
        const nouns = sentenceDoc.nouns().out('array');

        verbs.forEach((verb: string) => {
          const role: SemanticRole = {
            predicate: verb
          };

          // ARG0 (Agent) - ì²« ë²ˆì§¸ ëª…ì‚¬ë¥¼ ì£¼ì–´ë¡œ ê°„ì£¼
          if (nouns.length > 0) {
            const agentEntity = this.findEntityByText(nouns[0], entities);
            if (agentEntity) {
              role.agent = agentEntity;
            }
          }

          // ARG1 (Patient) - ë‘ ë²ˆì§¸ ëª…ì‚¬ë¥¼ ëª©ì ì–´ë¡œ ê°„ì£¼
          if (nouns.length > 1) {
            const patientEntity = this.findEntityByText(nouns[1], entities);
            if (patientEntity) {
              role.patient = patientEntity;
            }
          }

          // ìœ„ì¹˜, ì‹œê°„, ë°©ì‹ ë“±ì˜ ë¶€ê°€ ì—­í•  ì¶”ì¶œ
          this.extractAdditionalRoles(sentence, entities, role);

          if (role.agent || role.patient) {
            semanticRoles.push(role);
          }
        });
      });

      return semanticRoles;

    } catch (error) {
      console.error('Semantic role labeling ì˜¤ë¥˜:', error);
      return [];
    }
  }

  /**
   * ì¶”ê°€ì ì¸ ì˜ë¯¸ ì—­í• ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
   */
  private extractAdditionalRoles(sentence: string, entities: Entity[], role: SemanticRole): void {
    // ì‹œê°„ í‘œí˜„ ì¶”ì¶œ
    const timePatterns = [
      /(\d{4}ë…„)/g,
      /(ì˜¤ëŠ˜|ì–´ì œ|ë‚´ì¼|ì§€ê¸ˆ|í˜„ì¬)/g,
      /([ê°€-í£]+(?:ì‹œê°„|ë•Œ|ë™ì•ˆ|ê¸°ê°„))/g
    ];

    timePatterns.forEach(pattern => {
      const matches = sentence.match(pattern);
      if (matches) {
        const timeEntity = this.findEntityByText(matches[0], entities);
        if (timeEntity) {
          role.time = timeEntity;
        }
      }
    });

    // ì¥ì†Œ í‘œí˜„ ì¶”ì¶œ
    const locationPatterns = [
      /([ê°€-í£]+(?:ì—ì„œ|ì—|ìœ¼ë¡œ|ë¡œ))/g
    ];

    locationPatterns.forEach(pattern => {
      const matches = sentence.match(pattern);
      if (matches) {
        const locationEntity = this.findEntityByText(matches[0].replace(/ì—ì„œ|ì—|ìœ¼ë¡œ|ë¡œ/, ''), entities);
        if (locationEntity) {
          role.location = locationEntity;
        }
      }
    });
  }

  /**
   * ê´€ê³„ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
   */
  private async extractRelationships(
    entities: Entity[], 
    dependencies: Dependency[], 
    semanticRoles: SemanticRole[]
  ): Promise<Relationship[]> {
    const relationships: Relationship[] = [];

    try {
      // ì˜ë¯¸ ì—­í•  ê¸°ë°˜ ê´€ê³„ ì¶”ì¶œ
      semanticRoles.forEach(role => {
        if (role.agent && role.patient) {
          relationships.push({
            subject: role.agent,
            predicate: this.normalizePredicateToURI(role.predicate),
            object: role.patient,
            confidence: 0.80,
            context: `semantic_role_${role.predicate}`
          });
        }

        // ë¶€ê°€ ì—­í• ë“¤ë„ ê´€ê³„ë¡œ ë³€í™˜
        if (role.agent && role.location) {
          relationships.push({
            subject: role.agent,
            predicate: 'habitus33:locatedAt',
            object: role.location,
            confidence: 0.70,
            context: 'location_role'
          });
        }

        if (role.agent && role.time) {
          relationships.push({
            subject: role.agent,
            predicate: 'habitus33:occurredAt',
            object: role.time,
            confidence: 0.70,
            context: 'temporal_role'
          });
        }
      });

      // ì˜ì¡´ì„± ê¸°ë°˜ ê´€ê³„ ì¶”ì¶œ
      dependencies.forEach(dep => {
        const subjectEntity = this.findEntityByText(dep.token, entities);
        const objectEntity = this.findEntityByText(dep.head, entities);

        if (subjectEntity && objectEntity) {
          relationships.push({
            subject: subjectEntity,
            predicate: this.dependencyToURI(dep.relation),
            object: objectEntity,
            confidence: 0.65,
            context: `dependency_${dep.relation}`
          });
        }
      });

      // ì¤‘ë³µ ì œê±° ë° ì‹ ë¢°ë„ ê¸°ë°˜ ì •ë ¬
      return this.deduplicateRelationships(relationships);

    } catch (error) {
      console.error('Relationship extraction ì˜¤ë¥˜:', error);
      return [];
    }
  }

  /**
   * RDF íŠ¸ë¦¬í”Œì„ ìƒì„±í•©ë‹ˆë‹¤.
   * ğŸ¯ ì‚¬ìš©ì ì¤‘ì‹¬ ì§€ì‹ ì§„í™” ì¶”ì ì„ ìœ„í•œ í™•ì¥ëœ ë²„ì „
   */
  public async extractTriples(
    text: string, 
    modelName: string = 'advanced-nlp',
    contextBundle?: ContextBundle
  ): Promise<NewKnowledgeTriple[]> {
    try {
      const analysis = await this.analyzeText(text);
      const triples: NewKnowledgeTriple[] = [];

      // ê´€ê³„ ê¸°ë°˜ íŠ¸ë¦¬í”Œ ìƒì„±
      analysis.relationships.forEach(rel => {
        const baseTriple = {
          subject: rel.subject.uri || rel.subject.text,
          predicate: rel.predicate,
          object: rel.object.uri || rel.object.text,
          confidence: rel.confidence,
          source: modelName
        };
        
        triples.push(this.enrichTripleWithUserContext(baseTriple, contextBundle));
      });

      // ì—”í‹°í‹° íƒ€ì… íŠ¸ë¦¬í”Œ ìƒì„±
      analysis.entities.forEach(entity => {
        if (entity.uri) {
          const typeTriple = {
            subject: entity.uri,
            predicate: 'rdf:type',
            object: `habitus33:${entity.label}`,
            confidence: entity.confidence,
            source: modelName
          };
          
          triples.push(this.enrichTripleWithUserContext(typeTriple, contextBundle));

          // ë¼ë²¨ íŠ¸ë¦¬í”Œ ì¶”ê°€
          const labelTriple = {
            subject: entity.uri,
            predicate: 'rdfs:label',
            object: `"${entity.text}"@ko`,
            confidence: entity.confidence,
            source: modelName
          };
          
          triples.push(this.enrichTripleWithUserContext(labelTriple, contextBundle));
        }
      });

      return triples;

    } catch (error) {
      console.error('Triple extraction ì˜¤ë¥˜:', error);
      return [];
    }
  }

  /**
   * ğŸ¯ íŠ¸ë¦¬í”Œì— ì‚¬ìš©ì ë§¥ë½ ì •ë³´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
   * AdvancedTripleExtractorì—ì„œ ì¶”ì¶œëœ íŠ¸ë¦¬í”Œì— ëŒ€í•´ ì‚¬ìš©ì ì¤‘ì‹¬ ì¶”ì  ì •ë³´ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
   */
  private enrichTripleWithUserContext(
    triple: Partial<NewKnowledgeTriple>, 
    contextBundle?: ContextBundle
  ): NewKnowledgeTriple {
    const enriched: NewKnowledgeTriple = {
      subject: triple.subject || '',
      predicate: triple.predicate || '',
      object: triple.object || '',
      confidence: triple.confidence || 0.7,
      source: triple.source || 'advanced-nlp',
      sourceType: 'ai_assisted', // NLP ì¶”ì¶œì€ ê¸°ë³¸ì ìœ¼ë¡œ AI ë³´ì¡°
      derivedFromUser: false,
      evolutionStage: 'initial',
      temporalContext: new Date().toISOString()
    };

    if (contextBundle) {
      const userMemos = contextBundle.relevantNotes || [];
      
      if (userMemos.length > 0) {
        // ê³ ê¸‰ NLPë¡œ ì¶”ì¶œëœ ì—”í‹°í‹°ê°€ ì‚¬ìš©ì ë©”ëª¨ì— ìˆëŠ”ì§€ í™•ì¸
        const subjectInMemos = this.findTextInMemos(
          triple.subject?.replace(/^habitus33:/, '').replace(/_/g, ' ') || '', 
          userMemos
        );
        const objectInMemos = this.findTextInMemos(
          triple.object?.replace(/^habitus33:/, '').replace(/_/g, ' ') || '', 
          userMemos
        );

        if (subjectInMemos && objectInMemos) {
          // ì–‘ìª½ ì—”í‹°í‹° ëª¨ë‘ ì‚¬ìš©ì ë©”ëª¨ì—ì„œ ë°œê²¬ = ì‚¬ìš©ì ìˆœìˆ˜ ì—°ê²°
          enriched.sourceType = 'user_organic';
          enriched.derivedFromUser = true;
          enriched.confidence = Math.min((enriched.confidence || 0.7) + 0.25, 0.95); // ë†’ì€ ì‹ ë¢°ë„
          enriched.evolutionStage = 'synthesized'; // NLPê°€ ì‚¬ìš©ì ë©”ëª¨ ê°„ ì—°ê²°ì„ í•©ì„±
          enriched.originalMemoId = `memo_${userMemos.indexOf(subjectInMemos)}`;
        } else if (subjectInMemos || objectInMemos) {
          // í•œìª½ë§Œ ì‚¬ìš©ì ë©”ëª¨ì— ìˆìŒ = ë¶€ë¶„ì ìœ¼ë¡œ ì‚¬ìš©ì ê¸°ë°˜
          enriched.sourceType = 'ai_assisted';
          enriched.derivedFromUser = true;
          enriched.confidence = Math.min((enriched.confidence || 0.7) + 0.15, 0.85); // ì¤‘ê°„ ì‹ ë¢°ë„
          enriched.evolutionStage = 'gap_filled'; // ì§€ì‹ ê³µë°±ì„ NLPê°€ ì±„ì›€
        }
      }
    }

    return enriched;
  }

  /**
   * í…ìŠ¤íŠ¸ê°€ ì‚¬ìš©ì ë©”ëª¨ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
   */
  private findTextInMemos(searchText: string, memos: any[]): any | null {
    if (!searchText || searchText.trim().length < 2) {
      return null;
    }
    
    return memos.find(memo => 
      memo.content.toLowerCase().includes(searchText.toLowerCase()) ||
      searchText.toLowerCase().includes(memo.content.toLowerCase())
    ) || null;
  }

  // === ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤ ===

  private generateEntityId(text: string): string {
    return text.replace(/\s+/g, '_').replace(/[^a-zA-Z0-9_ê°€-í£]/g, '');
  }

  private findEntityByText(text: string, entities: Entity[]): Entity | undefined {
    return entities.find(entity => 
      entity.text === text || 
      entity.text.includes(text) || 
      text.includes(entity.text)
    );
  }

  private normalizePredicateToURI(predicate: string): string {
    const predicateMap: { [key: string]: string } = {
      'ê°œë°œ': 'habitus33:develops',
      'êµ¬í˜„': 'habitus33:implements',
      'ì„¤ëª…': 'habitus33:explains',
      'ë¶„ì„': 'habitus33:analyzes',
      'ì—°êµ¬': 'habitus33:researches',
      'í•™ìŠµ': 'habitus33:learns',
      'ì‚¬ìš©': 'habitus33:uses',
      'ì ìš©': 'habitus33:applies',
      'ìƒì„±': 'habitus33:creates',
      'ì œê³µ': 'habitus33:provides'
    };

    return predicateMap[predicate] || `habitus33:${predicate}`;
  }

  private dependencyToURI(relation: string): string {
    const relationMap: { [key: string]: string } = {
      'nsubj': 'habitus33:hasSubject',
      'dobj': 'habitus33:hasObject',
      'topic': 'habitus33:hasTopic',
      'subject': 'habitus33:hasSubject',
      'object': 'habitus33:hasObject',
      'location': 'habitus33:locatedAt',
      'manner': 'habitus33:performedBy'
    };

    return relationMap[relation] || `habitus33:${relation}`;
  }

  private deduplicateEntities(entities: Entity[]): Entity[] {
    const seen = new Set<string>();
    return entities.filter(entity => {
      const key = `${entity.text}_${entity.label}`;
      if (seen.has(key)) {
        return false;
      }
      seen.add(key);
      return true;
    }).sort((a, b) => a.startIndex - b.startIndex);
  }

  private deduplicateRelationships(relationships: Relationship[]): Relationship[] {
    const seen = new Set<string>();
    return relationships.filter(rel => {
      const key = `${rel.subject.text}_${rel.predicate}_${rel.object.text}`;
      if (seen.has(key)) {
        return false;
      }
      seen.add(key);
      return true;
    }).sort((a, b) => b.confidence - a.confidence);
  }

  private calculateOverallConfidence(
    entities: Entity[], 
    relationships: Relationship[], 
    dependencies: Dependency[]
  ): number {
    if (entities.length === 0 && relationships.length === 0) {
      return 0.0;
    }

    const entityConfidence = entities.reduce((sum, e) => sum + e.confidence, 0) / Math.max(entities.length, 1);
    const relationshipConfidence = relationships.reduce((sum, r) => sum + r.confidence, 0) / Math.max(relationships.length, 1);
    const dependencyBonus = Math.min(dependencies.length * 0.05, 0.2); // ì˜ì¡´ì„±ì´ ë§ì„ìˆ˜ë¡ ì‹ ë¢°ë„ ì¦ê°€

    return Math.min((entityConfidence + relationshipConfidence) / 2 + dependencyBonus, 1.0);
  }
} 